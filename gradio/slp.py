# -*- coding: utf-8 -*-
"""SLP_Assignment1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dm4yrZq1rOhswv1Kw-5fR10YYu-StzKt
"""

pip install gradio

import gradio as gr
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import T5ForConditionalGeneration, T5Tokenizer
import pandas as pd
from sklearn.model_selection import train_test_split

# Data extracted from the resume
data = {
    "question": [
        "What is the user's full name?",
        "What is the user's date of birth?",
        "What degree is the user pursuing?",
        "Which college does the user study in?",
        "What is the user's CGPA?",
        "Which programming languages does the user know?",
        "Which frameworks and libraries is the user familiar with?",
        "What databases does the user work with?",
        "What platforms does the user use?",
        "What are the user's key projects?",
        "Where can I find the user's GitHub profile?",
        "Where can I find the user's LinkedIn profile?"
    ],
    "sql_query": [
        "SELECT full_name FROM users WHERE id = 'Jeeva Ganesan';",
        "SELECT dob FROM users WHERE id = 'Jeeva Ganesan';",
        "SELECT degree FROM users WHERE id = 'Jeeva Ganesan';",
        "SELECT college FROM users WHERE id = 'Jeeva Ganesan';",
        "SELECT cgpa FROM academics WHERE user_id = 'Jeeva Ganesan';",
        "SELECT programming_languages FROM skills WHERE user_id = 'Jeeva Ganesan';",
        "SELECT frameworks_libraries FROM skills WHERE user_id = 'Jeeva Ganesan';",
        "SELECT databases FROM skills WHERE user_id = 'Jeeva Ganesan';",
        "SELECT platforms FROM skills WHERE user_id = 'Jeeva Ganesan';",
        "SELECT project_name FROM projects WHERE user_id = 'Jeeva Ganesan';",
        "SELECT github_link FROM profiles WHERE user_id = 'Jeeva Ganesan';",
        "SELECT linkedin_link FROM profiles WHERE user_id = 'Jeeva Ganesan';"
    ]
}

# Load dataset
df = pd.DataFrame(data)
X_train, X_test, y_train, y_test = train_test_split(df['question'], df['sql_query'], test_size=0.2, random_state=42)

# Load T5 model and tokenizer
model_name = 't5-small'
tokenizer = T5Tokenizer.from_pretrained(model_name)
model = T5ForConditionalGeneration.from_pretrained(model_name)

# Custom dataset
class SQLDataset(Dataset):
    def __init__(self, questions, queries, tokenizer, max_length=128):
        self.questions = questions
        self.queries = queries
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.questions)

    def __getitem__(self, idx):
        question = self.questions.iloc[idx]
        sql_query = self.queries.iloc[idx]

        input_tokens = self.tokenizer(question, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)
        target_tokens = self.tokenizer(sql_query, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)

        return {
            'input_ids': input_tokens['input_ids'].squeeze(),
            'attention_mask': input_tokens['attention_mask'].squeeze(),
            'labels': target_tokens['input_ids'].squeeze()
        }

# Train model
def train_model():
    train_dataset = SQLDataset(X_train, y_train, tokenizer)
    train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)
    model.train()
    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)
    epochs = 3

    for epoch in range(epochs):
        total_loss = 0
        for batch in train_loader:
            input_ids = batch['input_ids']
            attention_mask = batch['attention_mask']
            labels = batch['labels']

            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            total_loss += loss.item()

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        print(f"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}")

# Train and save model
train_model()
model.save_pretrained("t5_sql_parser")

def generate_sql(question):
    input_tokens = tokenizer(question, return_tensors='pt', padding=True, truncation=True)
    output = model.generate(**input_tokens)
    return tokenizer.decode(output[0], skip_special_tokens=True)

# Function to respond with full details
def respond_to_query(query):
    user_data = {
        "full_name": "Jeeva Ganesan",
        "dob": "07/09/2003",
        "degree": "B.Tech - Artificial Intelligence & Data Science",
        "college": "Karpagam College of Engineering",
        "cgpa": "7.42",
        "github_link": "https://github.com/JeevaganesanM",
        "linkedin_link": "https://www.linkedin.com/in/jeeva-ganesan-m-938b38278/"
    }
    return user_data.get(query, "Sorry, I don't have information for that query.")

# Gradio Interface
def gradio_interface(question):
    sql_query = generate_sql(question)
    response = respond_to_query(question)
    return sql_query, response

gui = gr.Interface(fn=gradio_interface, inputs=gr.Textbox(label="Enter a question"), outputs=[gr.Textbox(label="Generated SQL Query"), gr.Textbox(label="Response from User Data")])

gui.launch()